# Moral Machines

According to Susan Anderson, the “ultimate goal of machine ethics is to create autonomous ethical machines” (2007, 15). Extending on that, Van Wynsberghe and Robbins (2019, 721) define machine ethics as "a field of study dedicated to the computational entity as a
moral entity." These definitions give a foundation for discussing moral machines and artificial moral agents (AMA). These two terms have relatively similar meanings, both referring to an artificial moral entity capable of making its own moral decisions. For the sake of consistency, from now on this paper will refer to them as AMA.

The development of robots and AI with the sense of morality raises questions. For example, can they really be moral and should they be moral? Implementing moral theories is difficult. How does one take into account all the different nuances and variances of different cultures and religions. There is not any universal moral theory. For example, in the case of self-driving vehicles if the car should decide whether it would hit a cow or a person this would probably vary depending on if you were somewhere in Europe or somewhere in India. Should then different countries have different moral theories programmed into the machines or should the people buying the machines decide what values resemble their own? Even killing a person might be justified in some cases. For example, a person committing a shoot out in a crowded mall. If there exist no other way of stopping them than shooting them, and if this will save numerous human lives, it usually goes to that.

Allen and Wallach (2012, 59) argue that since moral theories are difficult to implement in machines a "virtue-based conception of morality" could be more achievable. This is because it is more well-suited for computational framework. Although we have advanced a great deal in artificial intelligence (AI) development we are still nowhere close to having AMAs with human level of moral reasoning. This is also partly the reason why there are not yet self-driving cars.

Important question to discuss is why do we want to develop AMAs. In their publication, Van Wynsberghe and Robbins (2019) list several key points that are usually used in justification of AMA development. These are inevitability, preventing harm to humans, complexity, public trust, prevent immoral usage, better morality and superior morality. It is argued that robots with capabilities of moral reasoning and decision making will be necessary in the future (Wallach, 2007). This could be, for example, if there are not enough people to take care of the elderly people. Robots with moral reasoning could also possibly prevent people from getting hurt. For example, a police robot might not have so fast trigger finger. This could save people that in encounters with human police would lose their life. It is also argued that when machines and algorithms become increasingly complex they turn into so-called black-boxes (Simon, 2010). When we are not sure how to machines makes its decisions how can we trust it? One proposed solution to this is implementing machine morality. That way we could trust the machine to make the morally right decisions. Also by implementing morality to machines public might be less worried about these soulless machines. With AMAs, immoral usage of thinking machines could be prevented to a greater extent. We can not, of course, know this for sure but maybe this would prevent the robots from committing acts of killing. Lastly, AMAs might be morally superior compared to us. This might not mean that they would have different views on how to treat people but rather that they would not commit immoral acts. In some ways we might be event able to learn something from them. Although the points presented above are simplifications of the possibly benefits they do offer perspective on development of AMAs. (Van Wynsberghe and Robbins, 2019)

Artificial moral agents do raise some questions though, such as, should robots be able to make wrong decisions? Humans do all the time and that is part of the moral process of deciding what is the right thing to do. Why should the robots not be able to do this? Is having a sense of moral about always making moral decisions? When talking about creating AMAs we are usually referring to machines that make always the right decision. Other interesting things separating AMAs from humans is the fear of consequence. We, the humans, commit immoral acts all the time. For example, people steal things. What stops most of us from stealing things is the fear and knowledge that we will have to face the consequences afterwards. These might include, for example, fines, prison or social isolation from peers. Would a robot then be held accountable for stealing a car or shooting a person? What about committing a robbery or assaulting a person? How should the robot be punished? Should we disassemble it? Put it to robot prison? Not oil it for a month? If a robot is considered an AMA should it not be itself who faces the consequences rather than the owner or the developer?

Van Wynsberghe and Robbins (2019) argue that robots should remain as joint cognitive systems (JCS). This means that even though robots might posses AI capabilities the human should always make the final decision. For example, a cancer detecting AI might greatly increase the rate cancers are detected from humans but the doctor should still in the end make the decision on what, when and how the patient should be treated. This also eliminates the issues of responsibility discussed earlier. If a robot makes a wrong decision or is misused the responsibility is on the shoulders of the owner or operator.

## References

Allen, C., & Wallach, W. (2011). Moral machines: Contradition in terms of abdication of human responsibility? In P. Lin, K. Abney, & G. A. Bekey (Eds.), Robot ethics: The ethical and social implications of robotics (pp. 55–68). Cambridge: MIT Press.

Anderson, M., & Anderson, S. L. (2007). Machine ethics: Creating an ethical intelligent agent. AI Magazine, 28(4), 15–26.

Simon, J. (2010). The entanglement of trust and knowledge on the Web. Ethics and Information Technology, 12(4), 343–355. https ://doi.org/10.1007/s1067 6-010-9243-5.

Van Wynsberghe, A. . & Robbins, S. . (2019) Critiquing the Reasons for Making Artificial Moral Agents. Science and engineering ethics. [Online] 25 (3), 719–735.

Wallach, W. (2007). Implementing moral decision making faculties in computers and robots. AI & Society, 22(4), 463–475. https ://doi.org/10.1007/s0014 6-007-0093-6.
